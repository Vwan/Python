{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入spidercomm 和 spiderbs4 模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import spiderbs4 as bs4\n",
    "import spidercomm as common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置所需的变量，创建输出结果路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up             \n",
    "url_root = 'http://www.jianshu.com/'\n",
    "url_seed = 'http://www.jianshu.com/p/e0bd6bfad10b?page=%d'\n",
    "spider_path='spider_res/bs4/'\n",
    "if os.path.exists(spider_path) == False:\n",
    "    os.makedirs(spider_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调用spidercomm的getNumberOfPages方法，判断要爬取的页面是否分页"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get total number of pages\n",
    "print \"url %s has multiple pages? %r\" % (url_seed,bs4.isMultiPaged(url_seed))\n",
    "page_count = bs4.getNumberOfPages(url_seed)\n",
    "print \"page_count is %s\" % page_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调用spidercomm的to_be_crawled_links方法，对所要爬取的页面获取所有链接，如果有分页，则分别获取各页并归并获取的链接，并最终将所有链接写入_all_links.txt文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get all links to be crawled and write to file    \n",
    "links_to_be_crawled=set()\n",
    "for count in range(page_count):\n",
    "    links = common.to_be_crawled_links(bs4.a_links(url_seed % count),count,url_root,url_seed)\n",
    "    print \"Total number of all links is %d\" % len(links)\n",
    "    links_to_be_crawled = links_to_be_crawled | links\n",
    "with open(spider_path+\"_all_links.txt\",'w+') as file:\n",
    "    file.write(\"\\n\".join(unicode(link).encode('utf-8',errors='ignore') for link in links_to_be_crawled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 循环所获取的链接列表，爬取每个链接的页面title和页面文章内容，分别写入以title命名的文件中，如无title，则写入Title_Is_None.txt文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# capture desired contents from crawled_urls\n",
    "if len(links_to_be_crawled) >= 1:\n",
    "    for link in links_to_be_crawled:           \n",
    "      title,content=bs4.crawled_page(link) \n",
    "      # print \"title is %s\" % title                \n",
    "      file_name = spider_path + title +'.txt'\n",
    "      common.writePage(file_name,content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
