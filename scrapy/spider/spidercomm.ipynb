{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 导入所需模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import urllib2\n",
    "import time\n",
    "import urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 下载页面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download(url,retry=2):\n",
    "   # print \"downloading %s\" % url\n",
    "    header = {\n",
    "            'User-Agent':'Mozilla/5.0'\n",
    "            }\n",
    "    try:\n",
    "        req = urllib2.Request(url,headers=header)\n",
    "        html = urllib2.urlopen(req).read()\n",
    "    except urllib2.HTTPError as e:\n",
    "            print \"download error: %s\" % e.reason\n",
    "            html = None\n",
    "            if retry >0:\n",
    "                print e.code\n",
    "                if hasattr(e,'code') and 500 <= e.code < 600:\n",
    "                    print e.code\n",
    "                    return download(url,retry-1)\n",
    "                    time.sleep(1)\n",
    "    return html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 将爬取内容写入文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writePage(filename,content):\n",
    "    content = unicode(content).encode('utf-8',errors='ignore')+\"\\n\"\n",
    "    if ('Title_Is_None.txt' in filename):\n",
    "        with open(filename,'a') as file:\n",
    "            file.write(content)\n",
    "    else:\n",
    "        with open(filename,'wb+') as file:\n",
    "            file.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 获取单一url的所有外链接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get urls to be crawled\n",
    "#:param alinks: list of tag 'a' href, dependent on implementation eg. bs4,lxml\n",
    "def to_be_crawled_link(alinks,url_seed,url_root):\n",
    "    links_to_be_crawled=set()\n",
    "    if len(alinks)==0:\n",
    "        return links_to_be_crawled\n",
    "    print \"len of alinks is %d\" % len(alinks)\n",
    "    for link in alinks:\n",
    "        link = link.get('href')            \n",
    "        if link != None and 'javascript:' not in link:\n",
    "            if link not in links_to_be_crawled:\n",
    "                realUrl = urlparse.urljoin(url_root,link)\n",
    "                links_to_be_crawled.add(realUrl)\n",
    "    return links_to_be_crawled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 获取指定分页的所有外连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_be_crawled_links(alinks,count,url_root,url_seed):\n",
    "    url = url_seed % count\n",
    "    links = to_be_crawled_link(alinks,url_root,url)#,{'class':'title'})\n",
    "    links.add(url)\n",
    "    return links"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
